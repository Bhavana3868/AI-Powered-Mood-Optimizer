# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DwLiphDReN0SrpFyH31ZBSJKk8xBcPmg
"""

!pip install transformers
!pip install deepface
!pip install librosa
!pip install scikit-learn
!pip install pydub
!pip install kagglehub
!pip install imbalanced-learn

# Import necessary libraries
from transformers import pipeline
from deepface import DeepFace
import librosa
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

# Function to upload audio and image files manually
def upload_files():
    from google.colab import files
    uploaded_files = files.upload()
    file_paths = list(uploaded_files.keys())
    return file_paths

# Function to get text emotion score
def get_text_emotion_score(text):
    nlp = pipeline('sentiment-analysis')
    result = nlp(text)
    text_score = result[0]['score'] if result[0]['label'] == 'POSITIVE' else 1 - result[0]['score']
    return float(text_score), result[0]['label']

# Function to get audio emotion score
def get_audio_emotion_score(audio_path, clf):
    y, sr = librosa.load(audio_path)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)
    features = np.hstack([mfccs, chroma, mel])
    # Predict using the pre-trained classifier
    emotion_prediction = clf.predict([features])
    audio_score = float(emotion_prediction[0])  # Get the predicted score and convert to float
    return audio_score

# Function to get facial emotion scores
def get_facial_emotion_scores(image_path):
    results = DeepFace.analyze(img_path=image_path, actions=['emotion'])
    if isinstance(results, dict):  # Single face detected
        return [results['emotion']]
    elif isinstance(results, list):  # Multiple faces detected
        return [res['emotion'] for res in results]

# Function to normalize facial emotion scores
def normalize_facial_scores(scores_list):
    combined_scores = {}
    for scores in scores_list:
        for emotion, score in scores.items():
            if emotion not in combined_scores:
                combined_scores[emotion] = 0.0  # Initialize with float type
            combined_scores[emotion] += float(score)  # Convert score to float

    # Calculate average for each emotion
    num_faces = len(scores_list)
    for emotion in combined_scores:
        combined_scores[emotion] /= num_faces

    # Find the dominant emotion and its score
    dominant_emotion = max(combined_scores, key=combined_scores.get)
    dominant_emotion_score = combined_scores[dominant_emotion]
    return dominant_emotion_score, dominant_emotion

# Function to integrate all emotion scores
def integrate_emotion_scores(text, audio_path, image_path, clf):
    # Fetch scores from text, audio, and facial analysis
    text_score, text_emotion = get_text_emotion_score(text)
    audio_score = get_audio_emotion_score(audio_path, clf)
    facial_score, facial_emotion = normalize_facial_scores(get_facial_emotion_scores(image_path))

    # Combine text, audio, and facial scores
    combined_score = (text_score + audio_score + facial_score) / 3

    # Find the emotion category with the highest score
    return combined_score, {
        'text': text_emotion, 'audio': audio_score, 'facial': facial_emotion
    }

# Example usage
# Upload audio and image files manually
file_paths = upload_files()

# Assign the paths accordingly
text_input = "I'm feeling great today!"
audio_path = [file for file in file_paths if file.endswith('.m4a')][0]  # Replace with your audio file path
image_path = [file for file in file_paths if file.endswith('.jpg') or file.endswith('.png')][0]  # Replace with your image file path

# Download the RAVDESS dataset using KaggleHub
import kagglehub
path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")
print("Path to dataset files:", path)

# Load and process the RAVDESS dataset
import os

def process_audio(file_path):
    y, sr = librosa.load(file_path)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)
    return np.hstack([mfccs, chroma, mel])

mfcc_features = []
labels = []

for root, _, files in os.walk(path):
    for filename in files:
        if filename.endswith(".wav"):
            file_path = os.path.join(root, filename)
            mfcc_features.append(process_audio(file_path))
            emotion = filename.split('-')[2]  # Assuming labels are in the filename
            labels.append(int(emotion))  # Ensure labels are integers

mfcc_features = np.array(mfcc_features)
labels = np.array(labels)
print("MFCC Features Shape:", mfcc_features.shape)
print("Labels Shape:", labels.shape)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(mfcc_features, labels)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train a RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Integrate emotion scores
final_emotion_score, emotions = integrate_emotion_scores(text_input, audio_path, image_path, clf)

# Print the final emotion scores and categories
print(f"Final Emotion Score: {final_emotion_score}")
print(f"Emotions: {emotions}")

# Import necessary libraries
from transformers import pipeline
from deepface import DeepFace
import librosa
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import SMOTE

# Function to upload audio and image files manually
def upload_files():
    from google.colab import files
    uploaded_files = files.upload()
    file_paths = list(uploaded_files.keys())
    return file_paths

# Function to get text emotion score
def get_text_emotion_score(text):
    nlp = pipeline('sentiment-analysis')
    result = nlp(text)
    text_score = result[0]['score'] if result[0]['label'] == 'POSITIVE' else 1 - result[0]['score']
    return float(text_score), result[0]['label']

# Function to get audio emotion score
def get_audio_emotion_score(audio_path, clf):
    y, sr = librosa.load(audio_path)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)
    features = np.hstack([mfccs, chroma, mel])
    # Predict using the pre-trained classifier
    emotion_prediction = clf.predict([features])
    audio_score = float(emotion_prediction[0])  # Get the predicted score and convert to float
    return audio_score

# Function to get facial emotion scores
def get_facial_emotion_scores(image_path):
    results = DeepFace.analyze(img_path=image_path, actions=['emotion'])
    if isinstance(results, dict):  # Single face detected
        return [results['emotion']]
    elif isinstance(results, list):  # Multiple faces detected
        return [res['emotion'] for res in results]

# Function to normalize facial emotion scores
def normalize_facial_scores(scores_list):
    combined_scores = {}
    for scores in scores_list:
        for emotion, score in scores.items():
            if emotion not in combined_scores:
                combined_scores[emotion] = 0.0  # Initialize with float type
            combined_scores[emotion] += float(score)  # Convert score to float

    # Calculate average for each emotion
    num_faces = len(scores_list)
    for emotion in combined_scores:
        combined_scores[emotion] /= num_faces

    # Find the dominant emotion and its score
    dominant_emotion = max(combined_scores, key=combined_scores.get)
    dominant_emotion_score = combined_scores[dominant_emotion]
    return dominant_emotion_score, dominant_emotion

# Function to integrate all emotion scores
def integrate_emotion_scores(text, audio_path, image_path, clf):
    # Fetch scores from text, audio, and facial analysis
    text_score, text_emotion = get_text_emotion_score(text)
    audio_score = get_audio_emotion_score(audio_path, clf)
    facial_score, facial_emotion = normalize_facial_scores(get_facial_emotion_scores(image_path))

    # Combine text, audio, and facial scores
    combined_score = (text_score + audio_score + facial_score) / 3

    # Find the emotion category with the highest score
    emotion_scores = {
        'text_score': text_score,
        'audio_score': audio_score,
        'facial_score': facial_score
    }
    emotion_labels = {
        'text_score': text_emotion,
        'audio_score': 'predicted_audio_emotion',  # Placeholder for actual audio emotion
        'facial_score': facial_emotion
    }
    final_emotion = emotion_labels[max(emotion_scores, key=emotion_scores.get)]

    return combined_score, final_emotion

# Define emotion-to-task mapping
emotion_to_task = {
    'happy': ['Take on a new project', 'Collaborate with a colleague', 'Lead a brainstorming session'],
    'sad': ['Focus on a familiar task', 'Take a short break', 'Listen to music'],
    'neutral': ['Continue with current tasks', 'Organize your workspace', 'Plan your day'],
    'angry': ['Take a deep breath', 'Go for a walk', 'Write down your thoughts'],
    'fear': ['Identify the source of fear', 'Discuss with a mentor', 'Engage in a calming activity'],
    'disgust': ['Clean your workspace', 'Take a break from the task', 'Reflect on positive experiences'],
    'surprise': ['Share the news with a colleague', 'Take a moment to process', 'Celebrate the surprise']
}

# Function to recommend tasks based on detected emotion
def recommend_tasks(emotion):
    # Recommend tasks based on the detected emotion
    recommended_tasks = emotion_to_task.get(emotion, ['No specific tasks available'])

    return recommended_tasks

# Example usage
# Upload audio and image files manually
file_paths = upload_files()

# Assign the paths accordingly
text_input = "I'm feeling great today!"
audio_path = [file for file in file_paths if file.endswith('.m4a')][0]  # Replace with your audio file path
image_path = [file for file in file_paths if file.endswith('.jpg') or file.endswith('.png')][0]  # Replace with your image file path

# Download the RAVDESS dataset using KaggleHub
import kagglehub
path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")
print("Path to dataset files:", path)

# Load and process the RAVDESS dataset
import os

def process_audio(file_path):
    y, sr = librosa.load(file_path)
    mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr).T, axis=0)
    mel = np.mean(librosa.feature.melspectrogram(y=y, sr=sr).T, axis=0)
    return np.hstack([mfccs, chroma, mel])

mfcc_features = []
labels = []

for root, _, files in os.walk(path):
    for filename in files:
        if filename.endswith(".wav"):
            file_path = os.path.join(root, filename)
            mfcc_features.append(process_audio(file_path))
            emotion = filename.split('-')[2]  # Assuming labels are in the filename
            labels.append(int(emotion))  # Ensure labels are integers

mfcc_features = np.array(mfcc_features)
labels = np.array(labels)
print("MFCC Features Shape:", mfcc_features.shape)
print("Labels Shape:", labels.shape)

# Balance the dataset using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(mfcc_features, labels)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train a RandomForestClassifier
clf = RandomForestClassifier(n_estimators=100)
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Integrate emotion scores and get the detected emotion
final_emotion_score, final_emotion = integrate_emotion_scores(text_input, audio_path, image_path, clf)
print(f"Final Emotion Score: {final_emotion_score}")
print(f"Final Emotion: {final_emotion}")

# Recommend tasks based on the detected emotion
recommended_tasks = recommend_tasks(final_emotion)
print(f"Recommended Tasks for '{final_emotion}': {recommended_tasks}")